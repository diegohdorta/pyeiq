---
layout: default
title: Running Examples
parent: Getting Started
nav_order: 3
---

# Running Samples
{: .no_toc }

1. TOC
{:toc}
---

The PyeIQ installation generates a tool manager called **pyeiq**. This tool can
be used to run the demos and applications, specify parameters, get info and more.

## PyeIQ Manager Tool

1. Start the manager tool:
```console
# pyeiq
```

2. The above command returns the PyeIQ manager tool options:
  * **pyeiq --list-apps**: List the available applications.
  * **pyeiq --list-demos**: List the available demos.
  * **pyeiq --run <app_name/demo_name>**: Run the application or demo.
    * E.g., # pyeiq --run object_detection_dnn.
  * **pyeiq --info <app_name/demo_name>**: app/demo short description and usage.
  * **pyeiq --clear-cache**: Clear cached media generated by demos.
    * E.g., # pyeiq --run object_detection_tflite.

### pyeiq --list-apps

1. Command to list the available applications:
```console
# pyeiq --list-apps
```

2. Expected output:
```console
########################################
#                                      #
#    PyeIQ - Available Applications    #
#                                      #
########################################
.
>>> switch_image
>>> switch_video
.
For more details about an application use --info (e.g., pyeiq --info switch_video).
To run an application use --run (e.g., pyeiq --run switch_image).
```

### pyeiq --list-demos

1. Command to list the available demos:
```console
# pyeiq --list-demos
```

2. Expected output:
```console
#################################
#                               #
#    PyeIQ - Available Demos    #
#                               #
#################################
.
>>> face_and_eyes_detection
>>> facial_expression_detection
>>> fire_classification_armnn
>>> fire_classification_tflite
>>> object_classification_tflite
>>> object_detection_dnn
>>> object_detection_tflite
>>> object_detection_yolov3
>>> pose_detection
.
For more details about a demo use --info (e.g.,pyeiq --info object_detection_tflite).
To run a demo use --run (e.g., pyeiq --run face_and_eyes_detection).
```

### pyeiq --run <app_name/demo_name>

1. Command to run the demo:
```console
# pyeiq --run object_detection_tflite
```

2. Expect output:
```console
#########################################
#                                       #
#    PyeIQ - object_detection_tflite    #
#                                       #
#########################################
.
INFO: Created TensorFlow Lite delegate for NNAPI.
Applied NNAPI delegate.
Running inference...
Image saved as /home/root/.cache/eiq/media/eIQObjectDetection.png
Done.
```

### pyeiq --info <app_name/demo_name>

1. Command to get the demo or application info:
```console
# pyeiq --info object_detection_tflite
```

2. Expected output:

```console
#########################################
#                                       #
#    PyeIQ - object_detection_tflite    #
#                                       #
#########################################

This demo uses:
   * TensorFlow Lite as inference engine.
   * Single Shot Detection as default algorithm.

Available parameters:

-d / --download: Chooses from which server the models are going to be downloaded.
Options: drive, github, wget. Default: Finds the fastest one.
# pyeiq --run object_detection_tflite --download drive

-i / --image: Run the demo on an image of your choice.
# pyeiq --run object_detection_tflite --image /home/root/image.jpg

-l / --labels: Uses a labels file of your choice within the demo.
Labels and models must be compatible for proper outputs.
# pyeiq --run object_detection_tflite --labels /home/root/labels.txt

-m / --model: Uses a model file of your choice within the demo.                                           
Labels and models must be compatible for proper outputs.                                                  
# pyeiq --run object_detection_tflite --model /home/root/model.tflite                                           

-r / --res: Choose the resolution of your video capture device.                                           
Options: full_hd (1920x1080), hd (1280x720), vga (640x480).                                               
Default: hd if supported, otherwise uses the best one available.                                          
# pyeiq --run object_detection_tflite --res vga                                                                 

-f / --video_fwk: Choose which framework is used to display the video.                                    
Options: opencv, v4l2, gstreamer (experimental). Default: v4l2.                                           
# pyeiq --run object_detection_tflite --video_fwk opencv                                                        

-v / --video_src: Run inference in videos.It can be from a video capture device or a video file.          
Options: True (Find a video capture device automatically), /dev/video<x>, path_to_video_file.             
# pyeiq --run object_detection_tflite --video_src /dev/video3                                                   

Multiple parameters can be used at once:                                                                  
# pyeiq --run object_detection_tflite -d drive -v True -f opencv -res vga      
```

### pyeiq --clear-cache

1. Command to clean the cache:
```console
# pyeiq --clear-cache
```

2. Expected output:
```console
Removing /home/root/.cache/eiq/media...
/home/root/.cache/eiq/media has been removed.
```

## Common Demos Parameters

 | Argument       | Description                                                                                                                                                                                                                                                               | Example of usage                                                                                                                            |
|----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|
| --download -d  | Choose from which server the models are going to download. Options: drive, github, wget. If none is specified, the application search automatically for the best server.                                                                                                  | /opt/eiq/demos# eiq_demo.py --download drive /opt/eiq/demos# eiq_demo.py -d github                                                          |
| --help -h      | Shows all available arguments for a certain demo and a brief explanation of its usage.                                                                                                                                                                                    | /opt/eiq/demos# eiq_demo.py --help /opt/eiq/demos# eiq_demo.py -h                                                                           |
| --image -i     | Use an image of your choice within the demo.                                                                                                                                                                                                                              | /opt/eiq/demos# eiq_demo.py --image /home/root/image.jpg /opt/eiq/demos# eiq_demo.py -i /home/root/image.jpg                                |
| --labels -l    | Use a labels file of your choice within the demo. Labels and models must be compatible for proper outputs.                                                                                                                                                                | /opt/eiq/demos# eiq_demo.py --labels /home/root/labels.txt /opt/eiq/demos# eiq_demo.py -l /home/root/labels.txt                             |
| --model -m     | Use a model file of your choice within the demo. Labels and models must be compatible for proper outputs.                                                                                                                                                                 | /opt/eiq/demos# eiq_demo.py --model /home/root/model.tflite /opt/eiq/demos# eiq_demo.py -m /home/root/model.tflite                          |
| --res -r       | Choose the resolution of your video capture device. Options: full_hd (1920x1080), hd (1280x720), vga (640x480). If none is specified, it uses hd as default. If your video device doesn't support the chosen resolution, it automatically selects the best one available. | /opt/eiq/demos# eiq_demo.py --res full_hd /opt/eiq/demos# eiq_demo.py -r vga                                                                |
| --video_fwk -f | Choose which video framework is used to display the video. Options: opencv, v4l2, gstreamer (need improvements). If none is specified, it uses v4l2 as default.                                                                                                           | /opt/eiq/demos# eiq_demo.py --video_fwk opencv /opt/eiq/demos# eiq_demo.py -f v4l2                                                          |
| --video_src -v | It makes the demo run inference on a video instead of an image. You can simply use the parameter "True" for it to run, specify your video capture device or even a video file. Options: True, /dev/video<x>, path_to_your_video_file.                                     | /opt/eiq/demos# eiq_demo.py --video_src /dev/video3 /opt/eiq/demos# eiq_demo.py -v True /opt/eiq/demos# eiq_demo.py -v /home/root/video.mp4 |


## Available Applications and Demos

See the [**Applications and Demos**][pyeiq_apps_demos] sections to check the available ones.

[pyeiq_apps_demos]: https://pyeiq.dev/2_applications_demos/2_0_applications_demos.html
